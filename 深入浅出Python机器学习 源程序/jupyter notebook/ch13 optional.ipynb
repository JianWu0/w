{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词数：8\n",
      "分词：{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumps': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "en = ['The quick brown fox jumps over a lazy dog']\n",
    "vect.fit(en)\n",
    "print('单词数：{}'.format(len(vect.vocabulary_)))\n",
    "print('分词：{}'.format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词数：1\n",
      "分词：{'那只敏捷的棕色狐狸跳过了一只懒惰的狗': 0}\n"
     ]
    }
   ],
   "source": [
    "cn = ['那只敏捷的棕色狐狸跳过了一只懒惰的狗']\n",
    "vect.fit(cn)\n",
    "print('单词数：{}'.format(len(vect.vocabulary_)))\n",
    "print('分词：{}'.format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache D:\\Temp\\jieba.cache\n",
      "Loading model cost 1.550 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['那 只 敏捷 的 棕色 狐狸 跳过 了 一只 懒惰 的 狗']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "cn = jieba.cut('那只敏捷的棕色狐狸跳过了一只懒惰的狗')\n",
    "cn = [' '.join(cn)]\n",
    "print(cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词数：6\n",
      "分词：{'敏捷': 2, '棕色': 3, '狐狸': 4, '跳过': 5, '一只': 0, '懒惰': 1}\n"
     ]
    }
   ],
   "source": [
    "vect.fit(cn)\n",
    "print('单词数：{}'.format(len(vect.vocabulary_)))\n",
    "print('分词：{}'.format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转化为词袋的特征：\n",
      "<1x6 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 6 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(cn)\n",
    "print('转化为词袋的特征：\\n{}'.format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词袋的密度表达：\n",
      "[[1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print('词袋的密度表达：\\n{}'.format(bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['懒惰 的 狐狸 不如 敏捷 的 狐狸 敏捷 , 敏捷 的 狐狸 不如 懒惰 的 狐狸 懒惰']\n"
     ]
    }
   ],
   "source": [
    "cn_1 = jieba.cut('懒惰的狐狸不如敏捷的狐狸敏捷,敏捷的狐狸不如懒惰的狐狸懒惰')\n",
    "cn2 = [' '.join(cn_1)]\n",
    "print(cn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转化为词袋的特征：\n",
      "<1x6 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3 stored elements in Compressed Sparse Row format>\n",
      "词袋的密度表达：\n",
      "[[0 3 3 0 4 0]]\n"
     ]
    }
   ],
   "source": [
    "new_bag = vect.transform(cn2)\n",
    "print('转化为词袋的特征：\\n{}'.format(repr(new_bag)))\n",
    "print('词袋的密度表达：\\n{}'.format(new_bag.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这句话的特征表达：\n",
      "[[1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "joke = jieba.cut('道士看见和尚亲吻了尼姑的嘴唇')\n",
    "joke = [' '.join(joke)]\n",
    "vect.fit(joke)\n",
    "joke_feature = vect.transform(joke)\n",
    "print('这句话的特征表达：\\n{}'.format(joke_feature.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这句话的特征表达：\n",
      "[[1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "joke2 = jieba.cut('尼姑看见道士的嘴唇亲吻了和尚')\n",
    "joke2 = [' '.join(joke2)]\n",
    "joke2_feature = vect.transform(joke2)\n",
    "print('这句话的特征表达：\\n{}'.format(joke2_feature.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "调整n-Gram参数后的词典：['亲吻 尼姑', '和尚 亲吻', '尼姑 嘴唇', '看见 和尚', '道士 看见']\n",
      "新的特征表达：[[1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "cv = vect.fit(joke)\n",
    "joke_feature = cv.transform(joke)\n",
    "print('调整n-Gram参数后的词典：{}'.format(cv.get_feature_names()))\n",
    "print('新的特征表达：{}'.format(joke_feature.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这句话的特征表达：\n",
      "[[0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "joke2 = jieba.cut('尼姑看见道士的嘴唇亲吻了和尚')\n",
    "joke2 = [' '.join(joke2)]\n",
    "joke2_feature = vect.transform(joke2)\n",
    "print('这句话的特征表达：\\n{}'.format(joke2_feature.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['亲吻', '和尚', '嘴唇', '尼姑', '看见', '道士']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer().fit(joke)\n",
    "print(tf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "卷 Windows 的文件夹 PATH 列表\n",
      "卷序列号为 00000169 2889:5EA9\n",
      "C:\\USERS\\CHAO\\DOCUMENTS\\JUPYTER NOTEBOOK\\ACLIMDB\n",
      "├─test\n",
      "│  ├─neg\n",
      "│  └─pos\n",
      "└─train\n",
      "    ├─neg\n",
      "    ├─pos\n",
      "    └─unsup\n"
     ]
    }
   ],
   "source": [
    "!tree ACLIMDB\n",
    "#请将aclImdb替换成你放置数据集的文件夹地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集文件数量:100\n",
      "\n",
      "随机抽一个看看:\n",
      " b\"All I could think of while watching this movie was B-grade slop. Many have spoken about it's redeeming quality is how this film portrays such a realistic representation of the effects of drugs and an individual and their subsequent spiral into a self perpetuation state of unfortunate events. Yet really, the techniques used (as many have already mentioned) were overused and thus unconvincing and irrelevant to the film as a whole.<br /><br />As far as the plot is concerned, it was lacklustre, unimaginative, implausible and convoluted. You can read most other reports on this film and they will say pretty much the same as I would.<br /><br />Granted some of the actors and actresses are attractive but when confronted with such boring action... looks can only carry a film so far. The action is poor and intermittent: a few punches thrown here and there, and a final gunfight towards the end. Nothing really to write home about.<br /><br />As others have said, 'BAD' movies are great to watch for the very reason that they are 'bad', you revel in that fact. This film, however, is a void. It's nothing.<br /><br />Furthermore, if one is really in need of an educational movie to scare people away from drug use then I would seriously recommend any number of other movies out there that board such issues in a much more effective way. 'Requiem For A Dream', 'Trainspotting', 'Fear and Loathing in Las Vegas' and 'Candy' are just a few examples. Though one should also check out some more lighthearted films on the same subject like 'Go' (overall, both serious and funny) and 'Halfbaked'.<br /><br />On a final note, the one possibly redeeming line in this movie, delivered by Vinnie Jones was stolen from 'Lock, Stock and Two Smokling Barrels'. To think that a bit of that great movie has been tainted by 'Loaded' is vile.<br /><br />Overall, I strongly suggest that you save you money and your time by NOT seeing this movie.\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "train_set = load_files('Imdblite/train/')\n",
    "X_train, y_train = train_set.data, train_set.target\n",
    "print('训练集文件数量:{}'.format(len(X_train)))\n",
    "print('\\n随机抽一个看看:\\n', X_train[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [doc.replace(b'<br />', b' ') for doc in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = load_files('Imdblite/test/')\n",
    "X_test, y_test = test.data, test.target\n",
    "X_test = [doc.replace(b'<br />', b' ') for doc in X_test]\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本特征数量：3941\n",
      "最后10个训练集样本特征：['young', 'your', 'yourself', 'yuppie', 'zappa', 'zero', 'zombie', 'zoom', 'zooms', 'zsigmond']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(X_train)\n",
    "X_train_vect = vect.transform(X_train)\n",
    "print('训练集样本特征数量：{}'.format(len(vect.get_feature_names())))\n",
    "print('最后10个训练集样本特征：{}'.format(vect.get_feature_names()[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型平均分：0.778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(LinearSVC(), X_train_vect, y_train)\n",
    "print('模型平均分：{:.3f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集模型得分：0.58\n"
     ]
    }
   ],
   "source": [
    "X_test_vect = vect.transform(X_test)\n",
    "clf = LinearSVC().fit(X_train_vect, y_train)\n",
    "print('测试集模型得分：{}'.format(clf.score(X_test_vect, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未经tfidf处理的特征：\n",
      " [[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "经过tfidf处理的特征：\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.13862307  0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(smooth_idf = False)\n",
    "tfidf.fit(X_train_vect)\n",
    "X_train_tfidf = tfidf.transform(X_train_vect)\n",
    "X_test_tfidf = tfidf.transform(X_test_vect)\n",
    "print('未经tfidf处理的特征：\\n',X_train_vect[:5,:5].toarray())\n",
    "print('经过tfidf处理的特征：\\n',X_train_tfidf[:5,:5].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经过tf-idf处理的训练集交叉验证得分：0.778\n",
      "经过tf-idf处理的测试集得分：0.580\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC().fit(X_train_tfidf, y_train)\n",
    "scores2 = cross_val_score(LinearSVC(), X_train_tfidf, y_train)\n",
    "\n",
    "print('经过tf-idf处理的训练集交叉验证得分：{:.3f}'.format(scores.mean()))\n",
    "print('经过tf-idf处理的测试集得分：{:.3f}'.format(clf.score(X_test_tfidf,\n",
    "                                                y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "停用词个数： 318\n",
      "列出前20个和最后20个：\n",
      " ['around', 'fifty', 'together', 'un', 'very', 'across', 'next', 'amongst', 'nor', 'first', 'more', 'its', 'de', 'serious', 'wherein', 'wherever', 'who', 'cry', 'full', 'after'] ['please', 'myself', 'himself', 'or', 'however', 'seems', 'almost', 'within', 'the', 'made', 'your', 'become', 'amoungst', 'all', 'him', 'had', 'already', 'least', 'it', 'anyone']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print('停用词个数：', len(ENGLISH_STOP_WORDS))\n",
    "print('列出前20个和最后20个：\\n', list(ENGLISH_STOP_WORDS)[:20],\n",
    "     list(ENGLISH_STOP_WORDS)[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去掉停用词后训练集交叉验证平均分：0.890\n",
      "去掉停用词后测试集模型得分：0.670\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(smooth_idf = False, stop_words = 'english')\n",
    "tfidf.fit(X_train)\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "scores3 = cross_val_score(LinearSVC(), X_train_tfidf, y_train)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "print('去掉停用词后训练集交叉验证平均分：{:.3f}'.format(scores3.mean()))\n",
    "print('去掉停用词后测试集模型得分：{:.3f}'.format(clf.score(X_test_tfidf, \n",
    "                                              y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
